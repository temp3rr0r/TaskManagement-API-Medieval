import os
from typing import List, Optional
from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
import ollama
from settings import settings

class RAGManager:
    def __init__(self):
        self.vector_store = None
        self.embeddings = OllamaEmbeddings(
            model=settings.OLLAMA_MODEL,
            base_url=settings.OLLAMA_HOST
        )
        self.initialize_vector_store()

    def load_documents(self) -> List[str]:
        """Load documents from PDF and EPUB files in the data directory"""
        documents = []
        data_dir = os.path.join(os.getcwd(), "data")
        
        if not os.path.exists(data_dir):
            print(f"Data directory not found at {data_dir}")
            return documents

        for filename in os.listdir(data_dir):
            file_path = os.path.join(data_dir, filename)
            try:
                if filename.lower().endswith('.pdf'):
                    loader = PyPDFLoader(file_path)
                    documents.extend(loader.load())
                    print(f"---------------------------------- Loaded PDF file: {filename}")
                elif filename.lower().endswith('.epub'):
                    loader = UnstructuredEPubLoader(file_path)
                    documents.extend(loader.load())
                    print(f"---------------------------------- Loaded EPUB file: {filename}")
            except Exception as e:
                print(f"Error loading file {filename}: {str(e)}")

        return documents

    def initialize_vector_store(self):
        """Initialize the vector store with document chunks"""
        documents = self.load_documents()
        
        if not documents:
            print("No documents loaded")
            return

        # Split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.RAG_CHUNK_SIZE,
            chunk_overlap=settings.RAG_CHUNK_OVERLAP,
            length_function=len,
        )
        chunks = text_splitter.split_documents(documents)

        # Create vector store
        self.vector_store = FAISS.from_documents(chunks, self.embeddings)
        print(f"Vector store initialized with {len(chunks)} chunks")

    def query_knowledge_base(self, query: str, k: int = settings.RAG_NUM_CHUNKS) -> str:
        """
        Query the knowledge base using RAG
        
        Args:
            query: The question to ask
            k: Number of relevant chunks to retrieve
            
        Returns:
            str: The answer generated by the LLM
        """
        if not self.vector_store:
            return "Knowledge base is not initialized. Please check if there are documents in the data directory."

        # Get relevant documents
        relevant_chunks = self.vector_store.similarity_search(query, k=k)
        
        # Prepare context from relevant chunks
        context = "\n".join([chunk.page_content for chunk in relevant_chunks])
        
        # Generate response using Ollama
        messages = [
            {
                "role": "system",
                "content": settings.SYSTEM_MESSAGES["knowledge_base"]
            },
            {
                "role": "user",
                "content": f"Based on the following context, please answer the question: {query}\n\nContext: {context}"
            }
        ]
        
        response = ollama.chat(
            model=settings.OLLAMA_MODEL,
            messages=messages
        )
        
        return response['message']['content']

# Create a singleton instance
rag_manager = RAGManager() 